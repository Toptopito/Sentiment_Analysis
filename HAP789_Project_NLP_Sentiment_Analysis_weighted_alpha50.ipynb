{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAP 789 Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and initial data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv('./data/TrainingRecords-4-4-2024.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dateCreated column\n",
    "df = df.drop(columns=['dateCreated'])\n",
    "\n",
    "# Remove duplicates based on commentId and keep the first occurrence\n",
    "df = df.drop_duplicates(subset='commentId', keep='first').reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values for comments\n",
    "df = df.dropna(subset=['comment'])\n",
    "\n",
    "# drop invalid comments\n",
    "# List of commentIds to drop\n",
    "commentIds_to_drop = [180459, 151656, 179845, 179923]\n",
    "\n",
    "# Drop rows with specified commentIds\n",
    "df = df[~df['commentId'].isin(commentIds_to_drop)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe that can be used for further processing\n",
    "main_df = df.copy()\n",
    "\n",
    "# Create comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment']\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove special characters and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re # for regular expressions\n",
    "\n",
    "# Remove punctuation, special characters, and numbers\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change all to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comment_processed column with lower case comments\n",
    "main_df['comment_processed'] = main_df['comment_processed'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load English language model with named entity recognition (NER) component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to remove proper nouns from text\n",
    "def remove_proper_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.ent_type_ == \"\"]\n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the remove_proper_nouns function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(remove_proper_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty comment_processed column\n",
    "main_df = main_df[main_df['comment_processed'].notnull() & (main_df['comment_processed'] != '')]\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with removed proper nouns\n",
    "main_df.to_csv('./data/checkpoint_01_no_proper.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing (complex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_01_no_proper.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Function to correct spelling mistakes in a text\n",
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = blob.correct()\n",
    "    return str(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the correct_spelling function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(correct_spelling)\n",
    "\n",
    "# Output the DataFrame with corrected spelling\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty comment_processed column\n",
    "main_df = main_df[main_df['comment_processed'].notnull() & (main_df['comment_processed'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_02_spell_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words \n",
    "(caution \"no\", \"not\" and other relevant negation words should not be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_02_spell_corrected.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English model from spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Convert the stop words into a DataFrame\n",
    "stop_words_df = pd.DataFrame({\"Stop_Words\": list(stop_words)})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "stop_words_df.to_csv(\"./data/spacy_english_stop_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re # for regular expressions\n",
    "\n",
    "# After processing the spacy stop word list create a custom stop word list and replace spacy stop words\n",
    "custom_stop_words_df = pd.read_csv(\"./data/spacy_english_stop_words_processed_new.csv\")\n",
    "\n",
    "# Remove punctuation, special characters, and numbers\n",
    "custom_stop_words_df['Stop_Words'] = custom_stop_words_df['Stop_Words'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)))\n",
    "\n",
    "# Create stop word list\n",
    "custom_stop_words = custom_stop_words_df['Stop_Words'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to remove custom stop words\n",
    "def remove_stop_words(text):\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Filter out the stop words\n",
    "    filtered_text = \" \".join(token.text for token in doc if token.text not in custom_stop_words)\n",
    "\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the stop word removal function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty comment_processed column\n",
    "main_df = main_df[main_df['comment_processed'].notnull() & (main_df['comment_processed'] != '')]\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_03_no_stop_words.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_03_no_stop_words.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize a single comment\n",
    "def lemmatize_comment(comment):\n",
    "    if comment == '' or comment is None:\n",
    "        return ''  # Return an empty string if it's NaN\n",
    "    \n",
    "    tokens = word_tokenize(comment)  # Tokenize the comment into words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize each word\n",
    "    return ' '.join(lemmatized_tokens)  # Join the lemmatized tokens back into a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the lemmatize function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(lemmatize_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty comment_processed column\n",
    "main_df = main_df[main_df['comment_processed'].notnull() & (main_df['comment_processed'] != '')]\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_04_lemmatized.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_04_lemmatized.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  # Adding this line to download the required \"omw-1.4\" resource\n",
    "\n",
    "# Function to find the most common synonym for a word using WordNet\n",
    "def most_common_synonym(word):\n",
    "    synsets = wordnet.synsets(word)  # Get all synsets for the word\n",
    "    if synsets:\n",
    "        all_synonyms = [syn.lemmas() for syn in synsets]  # Get all lemmas for each synset\n",
    "        all_synonyms = [lemma.name().replace('_', ' ') for syn in all_synonyms for lemma in syn]  # Flatten the list of lemmas, replacing underscores with spaces\n",
    "        synonym_counts = {synonym: all_synonyms.count(synonym) for synonym in all_synonyms}  # Count occurrences of each synonym\n",
    "        most_common_synonym = max(synonym_counts, key=synonym_counts.get)  # Get the synonym with the highest count\n",
    "        return most_common_synonym.lower()\n",
    "    else:\n",
    "        return word  # If no synsets found, return the original word\n",
    "\n",
    "# Function to replace each word in a comment with its most common synonym\n",
    "def replace_with_synonyms(comment):\n",
    "    tokens = word_tokenize(comment)  # Tokenize the comment into words\n",
    "    replaced_tokens = [most_common_synonym(token) for token in tokens]  # Replace each word with its most common synonym\n",
    "    return ' '.join(replaced_tokens)  # Join the replaced tokens back into a comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the synonym function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(replace_with_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with empty comment_processed column\n",
    "main_df = main_df[main_df['comment_processed'].notnull() & (main_df['comment_processed'] != '')]\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_05_synonyms.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word or phrase list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_05_synonyms.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word phrase list from text\n",
    "def generate_word_phrase_list(text):\n",
    "    text_list = text.split()\n",
    "    end_pos = len(text_list)\n",
    "    word_phrase_list = []\n",
    "    \n",
    "    for i in range(end_pos):\n",
    "        for j in range(i, end_pos):\n",
    "            words = text_list[i:j+1]\n",
    "            phrase = ' '.join(words)\n",
    "            word_phrase_list.append(phrase)\n",
    "    \n",
    "    return word_phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentId</th>\n",
       "      <th>comment</th>\n",
       "      <th>classification</th>\n",
       "      <th>comment_processed</th>\n",
       "      <th>word_phrase_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129687</td>\n",
       "      <td>Moral of the story while the nurses are all gr...</td>\n",
       "      <td>0</td>\n",
       "      <td>moral story nurse great body money</td>\n",
       "      <td>[moral, moral story, moral story nurse, moral ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>169075</td>\n",
       "      <td>If you are thinking about improving your appea...</td>\n",
       "      <td>0</td>\n",
       "      <td>think better appearance want competent</td>\n",
       "      <td>[think, think better, think better appearance,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88567</td>\n",
       "      <td>but I felt that my concerns were brushed aside...</td>\n",
       "      <td>1</td>\n",
       "      <td>feel concern brush aside go collapse</td>\n",
       "      <td>[feel, feel concern, feel concern brush, feel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>147104</td>\n",
       "      <td>My tear trough filler in my left eye looked li...</td>\n",
       "      <td>1</td>\n",
       "      <td>tear trough filler leave eye look like garage</td>\n",
       "      <td>[tear, tear trough, tear trough filler, tear t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137347</td>\n",
       "      <td>So, thank you Dr. Whitaker for all you have do...</td>\n",
       "      <td>0</td>\n",
       "      <td>thank encourage way</td>\n",
       "      <td>[thank, thank encourage, thank encourage way, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   commentId                                            comment  \\\n",
       "0     129687  Moral of the story while the nurses are all gr...   \n",
       "1     169075  If you are thinking about improving your appea...   \n",
       "2      88567  but I felt that my concerns were brushed aside...   \n",
       "3     147104  My tear trough filler in my left eye looked li...   \n",
       "4     137347  So, thank you Dr. Whitaker for all you have do...   \n",
       "\n",
       "   classification                              comment_processed  \\\n",
       "0               0             moral story nurse great body money   \n",
       "1               0         think better appearance want competent   \n",
       "2               1           feel concern brush aside go collapse   \n",
       "3               1  tear trough filler leave eye look like garage   \n",
       "4               0                            thank encourage way   \n",
       "\n",
       "                                    word_phrase_list  \n",
       "0  [moral, moral story, moral story nurse, moral ...  \n",
       "1  [think, think better, think better appearance,...  \n",
       "2  [feel, feel concern, feel concern brush, feel ...  \n",
       "3  [tear, tear trough, tear trough filler, tear t...  \n",
       "4  [thank, thank encourage, thank encourage way, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove missing values for comment_processed\n",
    "main_df = main_df.dropna(subset=['comment_processed'])\n",
    "\n",
    "# Apply the generate_word_phrase_list function to the comment_processed column to create word_phrase_list column\n",
    "main_df['word_phrase_list'] = main_df['comment_processed'].apply(generate_word_phrase_list)\n",
    "\n",
    "# Output the DataFrame with word phrase list\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_06_final_training_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final cleanups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop invalid comments from test selection\n",
    "# List of commentIds to drop\n",
    "commentIds_to_drop = [181673, 95658]\n",
    "\n",
    "# Drop rows with specified commentIds\n",
    "selection_df = main_df[~main_df['commentId'].isin(commentIds_to_drop)]\n",
    "\n",
    "# Check if each value in the 'comment_processed' column contains only one word\n",
    "is_single_word = selection_df['comment_processed'].str.split().apply(len) == 1\n",
    "\n",
    "# Keep rows where the comment_processed column has more than one word\n",
    "selection_df = selection_df[~is_single_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test and training sets and calculate similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentId</th>\n",
       "      <th>comment</th>\n",
       "      <th>classification</th>\n",
       "      <th>comment_processed</th>\n",
       "      <th>word_phrase_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>84833</td>\n",
       "      <td>The doctor talked to me with half his body in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>doctor talk body examination room</td>\n",
       "      <td>[doctor, doctor talk, doctor talk body, doctor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58035</td>\n",
       "      <td>And the registration person was not so nice.</td>\n",
       "      <td>1</td>\n",
       "      <td>registration person not nice</td>\n",
       "      <td>[registration, registration person, registrati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>190897</td>\n",
       "      <td>This is a pretend clinic they pretend to love ...</td>\n",
       "      <td>1</td>\n",
       "      <td>pretend clinic pretend love look fund</td>\n",
       "      <td>[pretend, pretend clinic, pretend clinic prete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173845</td>\n",
       "      <td>When I complained about it, I was asked to sen...</td>\n",
       "      <td>1</td>\n",
       "      <td>complain ask send picture</td>\n",
       "      <td>[complain, complain ask, complain ask send, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168214</td>\n",
       "      <td>Acts like a history teacher when really he's a...</td>\n",
       "      <td>1</td>\n",
       "      <td>act like history teacher dumb doctor keep ask ...</td>\n",
       "      <td>[act, act like, act like history, act like his...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   commentId                                            comment  \\\n",
       "0      84833  The doctor talked to me with half his body in ...   \n",
       "1      58035     And the registration person was not so nice.     \n",
       "2     190897  This is a pretend clinic they pretend to love ...   \n",
       "3     173845  When I complained about it, I was asked to sen...   \n",
       "4     168214  Acts like a history teacher when really he's a...   \n",
       "\n",
       "   classification                                  comment_processed  \\\n",
       "0               1                  doctor talk body examination room   \n",
       "1               1                       registration person not nice   \n",
       "2               1              pretend clinic pretend love look fund   \n",
       "3               1                          complain ask send picture   \n",
       "4               1  act like history teacher dumb doctor keep ask ...   \n",
       "\n",
       "                                    word_phrase_list  \n",
       "0  [doctor, doctor talk, doctor talk body, doctor...  \n",
       "1  [registration, registration person, registrati...  \n",
       "2  [pretend, pretend clinic, pretend clinic prete...  \n",
       "3  [complain, complain ask, complain ask send, co...  \n",
       "4  [act, act like, act like history, act like his...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where classification is 1\n",
    "complaints_df = selection_df[selection_df['classification'] == 1].sample(n=35, random_state=42)\n",
    "\n",
    "# Filter rows where classification is 0\n",
    "praises_df = selection_df[selection_df['classification'] == 0].sample(n=35, random_state=42)\n",
    "\n",
    "# Concatenate both dataframes\n",
    "test_df = pd.concat([complaints_df, praises_df])\n",
    "\n",
    "# Reset index of the resulting dataframe\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarity score\n",
    "def calculate_similarity_score(row, alpha, word_phrase_list, word_phrase_list_train):\n",
    "    cols_to_drop = ['commentId',\n",
    "                    'comment',\n",
    "                    'classification',\n",
    "                    'comment_processed',\n",
    "                    'word_phrase_list',\n",
    "                    'similarity_score']\n",
    "    \n",
    "    row_temp = row.drop(columns=cols_to_drop).copy()\n",
    "    row_temp = row_temp.reset_index(drop=True)\n",
    "\n",
    "    n_match = row_temp.iloc[0].sum() # number of matches\n",
    "    n_target_only = len(word_phrase_list) - n_match # number unmatched in target\n",
    "    n_train_only = len(word_phrase_list_train) - n_match # number unmatched in training\n",
    "\n",
    "    similarity_score = (n_match / (n_match + (alpha * n_target_only) + ((1-alpha) * n_train_only)))\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alpha for similarity score\n",
    "alpha = 0.5 # may need to change for sensitivity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress the PerformanceWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Create a folder for training files if it doesn't exist\n",
    "if not os.path.exists('./training_files'):\n",
    "    os.makedirs('./training_files')\n",
    "\n",
    "# Loop through each row in test_df\n",
    "for i in range(len(test_df)):\n",
    "    test_row = test_df.iloc[[i]] # get current row in dataframe format\n",
    "    \n",
    "    # Create a copy of main_df\n",
    "    train_df = main_df.copy()\n",
    "\n",
    "    # Remove the row with the same commentId as the current row in test_df\n",
    "    test_commentId = int(test_row['commentId'])\n",
    "\n",
    "    train_df = train_df[train_df['commentId'] != test_commentId]\n",
    "\n",
    "    # Get word_phrase_list of current row in test_df and create columns in train_df\n",
    "    word_phrase_list = test_row['word_phrase_list'][i]\n",
    "\n",
    "    for word_phrase in word_phrase_list:\n",
    "        train_df[word_phrase] = 0 # init to 0\n",
    "\n",
    "    # Set the word_phrase column to 1 if it exists in the training set row (exact match)\n",
    "    for word_phrase in word_phrase_list:\n",
    "        train_df[word_phrase] = train_df['word_phrase_list'].apply(lambda x: 1 if word_phrase in x else 0)\n",
    "        \n",
    "    # Remove rows where the sum of the columns created from word_phrase_list is 0\n",
    "    train_df = train_df[train_df[word_phrase_list].sum(axis=1) != 0]\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    train_df['similarity_score'] = 0.0 # initialize\n",
    "    \n",
    "    j = 0\n",
    "    for index, row in train_df.iterrows():\n",
    "        train_row = train_df.iloc[[j]] # get current row in dataframe format\n",
    "        word_phrase_list_train = train_row['word_phrase_list'][index]\n",
    "        train_df.at[index, 'similarity_score'] = calculate_similarity_score(train_row, alpha, word_phrase_list, word_phrase_list_train)\n",
    "        j += 1\n",
    "\n",
    "    # Get columns present in the word_phrase_list\n",
    "    word_phrase_columns = train_df.columns[train_df.columns.isin(word_phrase_list)]\n",
    "    \n",
    "    # Get all columns that are all ones or zeros within word_phrase_columns\n",
    "    columns_to_drop = word_phrase_columns[(train_df[word_phrase_columns].sum(axis=0) == len(train_df)) | (train_df[word_phrase_columns].sum(axis=0) == 0)]\n",
    "    \n",
    "    # Drop these columns with all ones or zeros (zero variance)\n",
    "    train_df = train_df.drop(columns=columns_to_drop)\n",
    "       \n",
    "    # Create filename\n",
    "    filename = f\"./training_files/weighted_alpha50/{test_commentId}.csv\"\n",
    "\n",
    "    # Write dataframe to CSV\n",
    "    train_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate \n",
    "def calculate_weight(sum_similarity_above_threshold, similarity):\n",
    "\n",
    "    # apply formula for weight\n",
    "    weight = similarity * (1 - sum_similarity_above_threshold)\n",
    "\n",
    "    # return zero if weight is less than of equal to 0\n",
    "    if weight > 0:\n",
    "        return weight\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop each row in test_df and calculate regression_weight\n",
    "i = 0\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    # get commentId\n",
    "    comment_id = row['commentId']\n",
    "\n",
    "    # open training file for test row\n",
    "    filename = f\"./training_files/weighted_alpha50/{comment_id}.csv\"\n",
    "    train_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # loop through each row of the dataframe to calculate regression_weights\n",
    "    j = 0\n",
    "    for index, row in train_df.iterrows():\n",
    "        train_row = train_df.iloc[[j]] # get current row in dataframe format\n",
    "\n",
    "        similarity = train_row['similarity_score'][index] # get similarity of current row\n",
    "        \n",
    "        # get sum of similarity score above similarity of current row\n",
    "        filtered_df = train_df[train_df['similarity_score'] > similarity]\n",
    "        sum_similarity_above_threshold = filtered_df['similarity_score'].sum()\n",
    "\n",
    "        # calculate the weights\n",
    "        train_df.at[index, 'regression_weight'] = calculate_weight(sum_similarity_above_threshold, similarity)\n",
    "        \n",
    "        j += 1\n",
    "\n",
    "    # Write dataframe to CSV\n",
    "    train_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "test_df.to_csv('./data/test_df_initial_weighted_alpha50.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Similarity Scores as Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate probability of complaint\n",
    "def calculate_probability(intercept, coefficients):\n",
    "    sumcoeff = intercept + np.sum(coefficients)\n",
    "    return 1 / (1 + np.exp(-sumcoeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Loop each row in test_df and calculate prediction, TP, TN, FP, FN\n",
    "i = 0\n",
    "cutoff = 0.5 # probability cutoff for prediction\n",
    "for index, row in test_df.iterrows():\n",
    "    # get commentId\n",
    "    comment_id = row['commentId']\n",
    "\n",
    "    # open training file for test row\n",
    "    filename = f\"./training_files/weighted_alpha50/{comment_id}.csv\"\n",
    "    train_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # set target variable\n",
    "    y = train_df['classification']\n",
    "\n",
    "    # set independent variables\n",
    "    cols_to_drop = ['classification',\n",
    "                    'commentId',\n",
    "                    'comment',\n",
    "                    'comment_processed',\n",
    "                    'word_phrase_list',\n",
    "                    'similarity_score',\n",
    "                    'regression_weight'\n",
    "                    ]\n",
    "    X = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # set weight to similarity score\n",
    "    sample_weights = train_df['regression_weight']\n",
    "\n",
    "    # Do logistic regression modeling with similarity_score as weight\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X, y, sample_weight = sample_weights)\n",
    "    \n",
    "    # create a column in the test_df for the predicted probability \n",
    "    test_df.at[index, 'probability'] = calculate_probability(log_reg.intercept_[0], log_reg.coef_[0]) \n",
    "\n",
    "    # calculate prediction from probability using cutoff\n",
    "    test_df.at[index, 'prediction'] = 1 if test_df.at[index, 'probability'] >= cutoff else 0\n",
    "\n",
    "    # calculate calibration\n",
    "    test_df.at[index, 'calibration'] = abs(test_df.at[index, 'probability'] - test_df.at[index, 'classification'])\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN\n",
    "    test_df.at[index, 'TP'] = 1 if (test_df.at[index, 'prediction'] == 1 and test_df.at[index, 'classification'] == 1) else 0\n",
    "    test_df.at[index, 'TN'] = 1 if (test_df.at[index, 'prediction'] == 0 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'FP'] = 1 if (test_df.at[index, 'prediction'] == 1 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'FN'] = 1 if (test_df.at[index, 'prediction'] == 0 and test_df.at[index, 'classification'] == 1) else 0\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Classification for Maximum Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop each row in test_df and calculate average classification for maximum match, AveClass\n",
    "cutoff = 0.5 # probability cutoff for prediction\n",
    "for index, row in test_df.iterrows():\n",
    "    # get commentId\n",
    "    comment_id = row['commentId']\n",
    "\n",
    "    # open training file for test row\n",
    "    filename = f\"./training_files/weighted_alpha50/{comment_id}.csv\"\n",
    "    train_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    cols_to_drop = ['commentId',\n",
    "                    'comment',\n",
    "                    'comment_processed',\n",
    "                    'word_phrase_list',\n",
    "                    'similarity_score',\n",
    "                    'regression_weight']\n",
    "    \n",
    "    # create dataframe without unneeded columns\n",
    "    train_df = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # create dataframe for sum calculation (number of matches)\n",
    "    sum_df = train_df.drop(columns=['classification'])\n",
    "\n",
    "    # Calculate the sum of each row\n",
    "    row_sums = sum_df.sum(axis=1)\n",
    "\n",
    "    # Find the maximum sum among all rows\n",
    "    max_sum = row_sums.max()\n",
    "\n",
    "    # Filter the DataFrame to retain only the rows where the sum is equal to the maximum sum\n",
    "    train_df = train_df[row_sums == max_sum]\n",
    "\n",
    "    # create a column in the test_df for average classification for maximum matches \n",
    "    test_df.at[index, 'AveClass'] = float(train_df['classification'].mean())\n",
    "\n",
    "    # calculate prediction from probability using cutoff\n",
    "    test_df.at[index, 'AveClassPred'] = 1 if test_df.at[index, 'AveClass'] >= cutoff else 0\n",
    "\n",
    "    # calculate calibration\n",
    "    test_df.at[index, 'AveClassCalib'] = abs(test_df.at[index, 'AveClass'] - test_df.at[index, 'classification'])\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN\n",
    "    test_df.at[index, 'AveClass_TP'] = 1 if (test_df.at[index, 'AveClassPred'] == 1 and test_df.at[index, 'classification'] == 1) else 0\n",
    "    test_df.at[index, 'AveClass_TN'] = 1 if (test_df.at[index, 'AveClassPred'] == 0 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'AveClass_FP'] = 1 if (test_df.at[index, 'AveClassPred'] == 1 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'AveClass_FN'] = 1 if (test_df.at[index, 'AveClassPred'] == 0 and test_df.at[index, 'classification'] == 1) else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Classification for Non-Zero Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop each row in test_df and calculate average classification for maximum match, AveClass\n",
    "cutoff = 0.5 # probability cutoff for prediction\n",
    "for index, row in test_df.iterrows():\n",
    "    # get commentId\n",
    "    comment_id = row['commentId']\n",
    "\n",
    "    # open training file for test row\n",
    "    filename = f\"./training_files/weighted_alpha50/{comment_id}.csv\"\n",
    "    train_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    cols_to_drop = ['commentId',\n",
    "                    'comment',\n",
    "                    'comment_processed',\n",
    "                    'word_phrase_list',\n",
    "                    'similarity_score']\n",
    "    \n",
    "    # create dataframe without unneeded columns\n",
    "    train_df = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Filter the DataFrame to retain only the rows where regression_weight is > 0\n",
    "    train_df = train_df[train_df['regression_weight'] > 0]\n",
    "\n",
    "    # create a column in the test_df for average classification for maximum matches \n",
    "    test_df.at[index, 'WeightClass'] = float(train_df['classification'].mean())\n",
    "\n",
    "    # calculate prediction from probability using cutoff\n",
    "    test_df.at[index, 'WeightClassPred'] = 1 if test_df.at[index, 'WeightClass'] >= cutoff else 0\n",
    "\n",
    "    # calculate calibration\n",
    "    test_df.at[index, 'WeightClassCalib'] = abs(test_df.at[index, 'WeightClass'] - test_df.at[index, 'classification'])\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN\n",
    "    test_df.at[index, 'WeightClass_TP'] = 1 if (test_df.at[index, 'WeightClassPred'] == 1 and test_df.at[index, 'classification'] == 1) else 0\n",
    "    test_df.at[index, 'WeightClass_TN'] = 1 if (test_df.at[index, 'WeightClassPred'] == 0 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'WeightClass_FP'] = 1 if (test_df.at[index, 'WeightClassPred'] == 1 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'WeightClass_FN'] = 1 if (test_df.at[index, 'WeightClassPred'] == 0 and test_df.at[index, 'classification'] == 1) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "test_df.to_csv('./data/test_df_completed_weighted_alpha50.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING REGRESSION PREDICTION\n",
      "CONFUSION MATRIX:\n",
      "TP: 19.0 | FP: 4.0\n",
      "FN: 16.0 | TN: 31.0\n",
      "\n",
      "\n",
      "Model accuracy: 71.43%\n",
      "Average of calibration: 0.32\n",
      "Standard deviation of calibration: 0.34\n"
     ]
    }
   ],
   "source": [
    "# For entire test_df calculate accuracy\n",
    "TP = test_df['TP'].sum()\n",
    "TN = test_df['TN'].sum()\n",
    "FP = test_df['FP'].sum()\n",
    "FN = test_df['FN'].sum()\n",
    "accuracy = round((TP + TN) / (TP + TN + FP + FN) * 100, 2)\n",
    "\n",
    "# Calculate average calibration and its standard deviation\n",
    "average_calibration = round(test_df['calibration'].mean(),2)\n",
    "std_dev_calibration = round(test_df['calibration'].std(),2)\n",
    "\n",
    "print(\"USING REGRESSION PREDICTION\")\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(f\"TP: {TP} | FP: {FP}\")\n",
    "print(f\"FN: {FN} | TN: {TN}\")\n",
    "print(f\"\\n\\nModel accuracy: {accuracy}%\")\n",
    "print(f\"Average of calibration: {average_calibration}\")\n",
    "print(f\"Standard deviation of calibration: {std_dev_calibration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING AVERAGE CLASSIFICATION FOR MAXIMUM MATCHES\n",
      "CONFUSION MATRIX:\n",
      "TP: 23.0 | FP: 3.0\n",
      "FN: 12.0 | TN: 32.0\n",
      "\n",
      "\n",
      "Model accuracy: 78.57%\n",
      "Average of calibration: 0.25\n",
      "Standard deviation of calibration: 0.33\n"
     ]
    }
   ],
   "source": [
    "# For entire test_df calculate accuracy\n",
    "TP = test_df['AveClass_TP'].sum()\n",
    "TN = test_df['AveClass_TN'].sum()\n",
    "FP = test_df['AveClass_FP'].sum()\n",
    "FN = test_df['AveClass_FN'].sum()\n",
    "accuracy = round((TP + TN) / (TP + TN + FP + FN) * 100, 2)\n",
    "\n",
    "# Calculate average calibration and its standard deviation\n",
    "average_calibration = round(test_df['AveClassCalib'].mean(),2)\n",
    "std_dev_calibration = round(test_df['AveClassCalib'].std(),2)\n",
    "\n",
    "print(\"USING AVERAGE CLASSIFICATION FOR MAXIMUM MATCHES\")\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(f\"TP: {TP} | FP: {FP}\")\n",
    "print(f\"FN: {FN} | TN: {TN}\")\n",
    "print(f\"\\n\\nModel accuracy: {accuracy}%\")\n",
    "print(f\"Average of calibration: {average_calibration}\")\n",
    "print(f\"Standard deviation of calibration: {std_dev_calibration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING AVERAGE CLASSIFICATION FOR NON-ZERO WEIGHTS\n",
      "CONFUSION MATRIX:\n",
      "TP: 21.0 | FP: 2.0\n",
      "FN: 14.0 | TN: 33.0\n",
      "\n",
      "\n",
      "Model accuracy: 77.14%\n",
      "Average of calibration: 0.32\n",
      "Standard deviation of calibration: 0.32\n"
     ]
    }
   ],
   "source": [
    "# For entire test_df calculate accuracy\n",
    "TP = test_df['WeightClass_TP'].sum()\n",
    "TN = test_df['WeightClass_TN'].sum()\n",
    "FP = test_df['WeightClass_FP'].sum()\n",
    "FN = test_df['WeightClass_FN'].sum()\n",
    "accuracy = round((TP + TN) / (TP + TN + FP + FN) * 100, 2)\n",
    "\n",
    "# Calculate average calibration and its standard deviation\n",
    "average_calibration = round(test_df['WeightClassCalib'].mean(),2)\n",
    "std_dev_calibration = round(test_df['WeightClassCalib'].std(),2)\n",
    "\n",
    "print(\"USING AVERAGE CLASSIFICATION FOR NON-ZERO WEIGHTS\")\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(f\"TP: {TP} | FP: {FP}\")\n",
    "print(f\"FN: {FN} | TN: {TN}\")\n",
    "print(f\"\\n\\nModel accuracy: {accuracy}%\")\n",
    "print(f\"Average of calibration: {average_calibration}\")\n",
    "print(f\"Standard deviation of calibration: {std_dev_calibration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
