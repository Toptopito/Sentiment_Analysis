{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAP 789 Sentiment Analysis Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and initial data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv('./data/TrainingRecords-4-4-2024.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop dateCreated column\n",
    "df = df.drop(columns=['dateCreated'])\n",
    "\n",
    "# Remove duplicates based on commentId and keep the first occurrence\n",
    "df = df.drop_duplicates(subset='commentId', keep='first').reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing values for comments\n",
    "df = df.dropna(subset=['comment'])\n",
    "\n",
    "# drop invalid comments\n",
    "# List of commentIds to drop\n",
    "commentIds_to_drop = [180459, 151656, 179845, 179923]\n",
    "\n",
    "# Drop rows with specified commentIds\n",
    "df = df[~df['commentId'].isin(commentIds_to_drop)]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe that can be used for further processing\n",
    "main_df = df.copy()\n",
    "\n",
    "# Create comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment']\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import spacy\n",
    "\n",
    "# Load English language model with named entity recognition (NER) component\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to remove proper nouns from text\n",
    "def remove_proper_nouns(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_tokens = [token.text for token in doc if token.ent_type_ == \"\"]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply the remove_proper_nouns function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(remove_proper_nouns)\n",
    "\n",
    "# Output the DataFrame with no proper nouns\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with removed proper nouns\n",
    "main_df.to_csv('./data/checkpoint_01_no_proper.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing (simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_01_no_proper.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comment_processed column with lower case comments\n",
    "main_df['comment_processed'] = main_df['comment_processed'].str.lower()\n",
    "\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import re # for regular expressions\n",
    "\n",
    "# Remove punctuation, special characters, and numbers\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', str(x)))\n",
    "\n",
    "main_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing (complex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Function to correct spelling mistakes in a text\n",
    "def correct_spelling(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected_text = blob.correct()\n",
    "    return str(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the correct_spelling function to the comment_processed column\n",
    "main_df['comment_processed'] = main_df['comment_processed'].apply(correct_spelling)\n",
    "\n",
    "# Output the DataFrame with corrected spelling\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_02_spell_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words (caution \"not\", \"not\" and other relevant words should not be removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_02_spell_corrected.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and remove proper nouns phase 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word or phrase list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate word phrase list from text\n",
    "def generate_word_phrase_list(text):\n",
    "    text_list = text.split()\n",
    "    end_pos = len(text_list)\n",
    "    word_phrase_list = []\n",
    "    \n",
    "    for i in range(end_pos):\n",
    "        for j in range(i, end_pos):\n",
    "            words = text_list[i:j+1]\n",
    "            phrase = ' '.join(words)\n",
    "            word_phrase_list.append(phrase)\n",
    "    \n",
    "    return word_phrase_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values for comment_processed\n",
    "main_df = main_df.dropna(subset=['comment_processed'])\n",
    "\n",
    "# Apply the generate_word_phrase_list function to the comment_processed column to create word_phrase_list column\n",
    "main_df['word_phrase_list'] = main_df['comment_processed'].apply(generate_word_phrase_list)\n",
    "\n",
    "# Output the DataFrame with word phrase list\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "main_df.to_csv('./data/checkpoint_06_with_word_phrase_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint file\n",
    "main_df = pd.read_csv('./data/checkpoint_06_with_word_phrase_list.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test and training sets and calculate similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where classification is 1\n",
    "complaints_df = main_df[main_df['classification'] == 1].sample(n=35, random_state=42)\n",
    "\n",
    "# Filter rows where classification is 0\n",
    "praises_df = main_df[main_df['classification'] == 0].sample(n=35, random_state=42)\n",
    "\n",
    "# Concatenate both dataframes\n",
    "test_df = pd.concat([complaints_df, praises_df])\n",
    "\n",
    "# Reset index of the resulting dataframe\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate similarity score\n",
    "def calculate_similarity_score(row, alpha, word_phrase_list):\n",
    "    n_match = row[word_phrase_list].sum(axis=1) # number of matches\n",
    "    n_target_only = len(word_phrase_list) - n_match # number unmatched in target\n",
    "    n_train_only = len(row['word_phrase_list']) - n_match # number unmatched in training\n",
    "\n",
    "    similarity_score = (n_match / (n_match + (alpha * n_target_only) + ((1-alpha) * n_train_only)))\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set alpha for similarity score\n",
    "alpha = 0.8 # may need to change for sensitivity testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress the PerformanceWarning\n",
    "warnings.filterwarnings(\"ignore\", message=\"DataFrame is highly fragmented\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Create a folder for training files if it doesn't exist\n",
    "if not os.path.exists('./training_files'):\n",
    "    os.makedirs('./training_files')\n",
    "\n",
    "# Loop through each row in test_df\n",
    "for i in range(len(test_df)):\n",
    "    test_row = test_df.iloc[[i]] # get current row in dataframe format\n",
    "    \n",
    "    # Create a copy of main_df\n",
    "    train_df = main_df.copy()\n",
    "\n",
    "    # Remove the row with the same commentId as the current row in test_df\n",
    "    test_commentId = int(test_row['commentId'])\n",
    "    train_df = train_df[train_df['commentId'] != test_commentId]\n",
    "\n",
    "    # Get word_phrase_list of current row in test_df and create columns in train_df\n",
    "    print(f\"\\ntest: {i}\") \n",
    "    print(f\"len: {len(test_row['word_phrase_list'])}\")\n",
    "    print(f\"list[{i}]: {test_row['word_phrase_list'][i]}\")\n",
    "    word_phrase_list = test_row['word_phrase_list'][i]\n",
    "    for word_phrase in word_phrase_list:\n",
    "        train_df[word_phrase] = 0 # init to 0\n",
    "\n",
    "    # Set the word_phrase column to 1 if it exists in the training set row (exact match)\n",
    "    for word_phrase in word_phrase_list:\n",
    "        train_df[word_phrase] = train_df['word_phrase_list'].apply(lambda x: 1 if word_phrase in x else 0)\n",
    "        \n",
    "    # Remove rows where the sum of the columns created from word_phrase_list is 0\n",
    "    train_df = train_df[train_df[word_phrase_list].sum(axis=1) != 0]\n",
    "\n",
    "    # Calculate similarity scores\n",
    "    train_df['similarity_score'] = 0.0 # initialize\n",
    "    \n",
    "    j = 0\n",
    "    for index, row in train_df.iterrows():\n",
    "        train_row = train_df.iloc[[j]] # get current row in dataframe format\n",
    "        train_df.at[index, 'similarity_score'] = calculate_similarity_score(train_row, alpha, word_phrase_list)\n",
    "        j += 1\n",
    "    \n",
    "    # Get columns present in the word_phrase_list\n",
    "    word_phrase_columns = train_df.columns[train_df.columns.isin(word_phrase_list)]\n",
    "    \n",
    "    # Get all columns that are all ones or zeros within word_phrase_columns\n",
    "    columns_to_drop = word_phrase_columns[(train_df[word_phrase_columns].sum(axis=0) == len(train_df)) | (train_df[word_phrase_columns].sum(axis=0) == 0)]\n",
    "    \n",
    "    # Drop these columns with all ones or zeros (zero variance)\n",
    "    train_df = train_df.drop(columns=columns_to_drop)\n",
    "       \n",
    "    # Create filename\n",
    "    filename = f\"./training_files/{test_commentId}.csv\"\n",
    "\n",
    "    # Write dataframe to CSV\n",
    "    train_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "test_df.to_csv('./data/test_df_initial.csv', index=False)\n",
    "main_df.to_csv('./data/checkpoint_07_pre_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Similarity Scores as Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint files\n",
    "test_df = pd.read_csv('./data/test_df_initial.csv', low_memory=False)\n",
    "main_df = pd.read_csv('./data/checkpoint_07_pre_regression.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate probability of complaint\n",
    "def calculate_probability(intercept, coefficients):\n",
    "    sumcoeff = intercept + np.sum(coefficients)\n",
    "    return 1 / (1 + np.exp(-sumcoeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Function for model training using logistic regression\n",
    "def calculate_probability(test_row):\n",
    "\n",
    "    # open training file for test row\n",
    "    commentId = int(test_row['commentId'])\n",
    "    filename = f\"./training_files/{test_commentId}.csv\"\n",
    "    train_df = pd.read_csv(filename, low_memory=False)\n",
    "\n",
    "    # set target variable\n",
    "    y = train_df['classification']\n",
    "\n",
    "    # set independent variables\n",
    "    cols_to_drop = ['classification',\n",
    "                    'commentId',\n",
    "                    'comment',\n",
    "                    'comment_processed',\n",
    "                    'word_phrase_list',\n",
    "                    'similarity_score']\n",
    "    X = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # set weight to similarity score\n",
    "    sample_weights = train_df['similarity_score']\n",
    "\n",
    "    # Do logistic regression modeling with similarity_score as weight\n",
    "    log_reg = LogisticRegression()\n",
    "    log_reg.fit(X, y, sample_weight = sample_weights)\n",
    "\n",
    "    probability = calculate_probability(log_reg.intercept_[0], log_reg.coef_[0])\n",
    "\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop each row in test_df and calculate prediction, TP, TN, FP, FN\n",
    "i = 0\n",
    "cutoff = 0.5 # probability cutoff for prediction\n",
    "for index, row in test_df.iterrows():\n",
    "    test_row = test_df.iloc[[i]] # get current row in dataframe format\n",
    "    \n",
    "    # create a column in the test_df for the predicted probability \n",
    "    test_df.at[index, 'probability'] = calculate_probability(test_row) \n",
    "\n",
    "    # calculate prediction from probability using cutoff\n",
    "    test_df.at[index, 'prediction'] = 1 if test_df.at[index, 'probability'] >= cutoff else 0\n",
    "\n",
    "    # calculate calibration\n",
    "    test_df.at[index, 'calibration'] = abs(test_df.at[index, 'probability'] - test_df.at[index, 'classification'])\n",
    "    \n",
    "    # Calculate TP, TN, FP, FN\n",
    "    test_df.at[index, 'TP'] = 1 if (test_df.at[index, 'prediction'] == 1 and test_df.at[index, 'classification'] == 1) else 0\n",
    "    test_df.at[index, 'TN'] = 1 if (test_df.at[index, 'prediction'] == 0 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'FP'] = 1 if (test_df.at[index, 'prediction'] == 1 and test_df.at[index, 'classification'] == 0) else 0\n",
    "    test_df.at[index, 'FN'] = 1 if (test_df.at[index, 'prediction'] == 0 and test_df.at[index, 'classification'] == 1) else 0\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store file with corrected spelling for data checkpoint purposes\n",
    "test_df.to_csv('./data/test_df_completed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "\n",
    "# read checkpoint files\n",
    "test_df = pd.read_csv('./data/test_df_completed.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For entire test_df calculate accuracy\n",
    "TP = test_df['TP'].sum()\n",
    "TN = test_df['TN'].sum()\n",
    "FP = test_df['FP'].sum()\n",
    "FN = test_df['FN'].sum()\n",
    "accuracy = round((TP + TN) / (TP + TN + FP + FN) * 100, 2)\n",
    "\n",
    "# Calculate average calibration and its standard deviation\n",
    "average_calibration = round(test_df['calibration'].mean(),2)\n",
    "std_dev_calibration = round(test_df['calibration'].std(),2)\n",
    "\n",
    "\n",
    "print(f\"Model accuracy: {accuracy}%\")\n",
    "print(f\"Average of calibration: {average_calibration}\")\n",
    "print(f\"Standard deviation of calibration: {std_dev_calibration}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
